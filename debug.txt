1. Check the Log-Likelihood Value
Print out the log-likelihood value before and after the optimization to see if there was any change. This will tell you if the optimizer is actually making progress.

python
Copy code
log_likelihood_before = model_est.get_log_likelihood()[0]
beta_est = model_est.solve_for_optimal_beta(verbose=True)
log_likelihood_after = model_est.get_log_likelihood()[0]

print(f"Log-likelihood before optimization: {log_likelihood_before}")
print(f"Log-likelihood after optimization: {log_likelihood_after}")
print(f"Estimated beta: {beta_est}")
2. Experiment with Different Initial Values
Try different initial values for beta_est_init to see if the optimization behaves differently. If the optimizer always returns the initial values, it might indicate a deeper issue.

3. Relax Convergence Criteria
You might want to check and possibly relax the convergence criteria (tolerances) in the optimizer's settings. This could allow the optimizer to make further progress.

4. Visualize the Gradient
If possible, inspect or print out the gradients during the optimization process to see if they are too small or if they indicate no direction for improvement.

5. Check the Data
Ensure that the data you are using is sufficiently varied and informative for the model to learn meaningful parameters. Sometimes, if the data doesn't vary much, the optimizer might not find a way to improve the parameters.

6. Review the Implementation
Double-check the implementation, particularly how the obs_index_list and network_struct are created. Any issues in these might lead to an ineffective optimization process.

By following these steps, you should be able to determine why the optimization isn't making progress and adjust accordingly.